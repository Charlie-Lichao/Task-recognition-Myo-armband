{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from utilities import *\n",
    "from features import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation method\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import f1_score\n",
    "from itertools import cycle\n",
    "from scipy import interp\n",
    "from itertools import chain\n",
    "\n",
    "#model\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tensorflow.python.ops import rnn, rnn_cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readme:<br/>\n",
    "\n",
    "The first two folds, 'Read whole data...' and 'Feature extraction' can be skipped if you do not want to change the window size, filter frequency and features. The default window size is setting as 10, emg filter with 20 Hz cutoff frequency, and 12 type of features.<br/>\n",
    "\n",
    "However, if you want to change any of these parameters, you should open first two folds and check the utilities.py or features.py to modify them. <br/>\n",
    "\n",
    "Therefore, you can go straight to the third fold, 'Data Spliting'. Run the first cell inside this fold to read the features you want to test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Read whole data and store them in seperate files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### This cell code is store the dataset after filter and fft process, and they are already exsit in the data files.\n",
    "\n",
    "   So it is not necessary to run this cell unless the datasets have been deleted or the filter frequency want to be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for j in range (1,33):\n",
    "    file_prefix = './data/Trial'\n",
    "    acc_df = pd.read_csv(file_prefix + str(j) + '/Myo_accelerometer.csv',index_col=None, header=None)\n",
    "\n",
    "    acc_temp = acc_df.values\n",
    "    acc_dataOut = np.zeros((len(acc_temp),3))\n",
    "    acc_dataOut_fft = np.zeros((len(acc_temp),3))\n",
    "    for p in range(3):\n",
    "        acc_dataOut[:,p] = butter_lowpass_filter(acc_df.iloc[:,p+1], cutoff, fss, order) #cutoff\n",
    "        acc_dataOut[:,p] = acc_dataOut[:,p]/max(abs(acc_dataOut[:,p]))\n",
    "        acc_dataOut_fft[:,p] = np.fft.fft(acc_dataOut[:,p]).real\n",
    "\n",
    "    acc_data_filter = np.concatenate([acc_temp[:,0:1], acc_dataOut], axis=1)\n",
    "    acc_data_filter_fft = np.concatenate([acc_temp[:,0:1], acc_dataOut_fft], axis=1)\n",
    "\n",
    "    acc_df = pd.DataFrame.from_records(acc_data_filter)\n",
    "    acc_df_fft = pd.DataFrame.from_records(acc_data_filter_fft)\n",
    "\n",
    "\n",
    "\n",
    "    gyro_df = pd.read_csv(file_prefix + str(j) + '/Myo_gyro.csv',index_col=None, header=None)\n",
    "\n",
    "    gyro_temp = gyro_df.values\n",
    "    gyro_dataOut = np.zeros((len(gyro_temp),3))\n",
    "    gyro_dataOut_fft = np.zeros((len(gyro_temp),3))\n",
    "    for p in range(3):\n",
    "        gyro_dataOut[:,p] = butter_lowpass_filter(gyro_df.iloc[:,p+1], cutoff, fss, order)\n",
    "        gyro_dataOut[:,p] = gyro_dataOut[:,p]/max(abs(gyro_dataOut[:,p]))\n",
    "        gyro_dataOut_fft[:,p] = np.fft.fft(gyro_dataOut[:,p]).real\n",
    "\n",
    "    gyro_data_filter = np.concatenate([gyro_temp[:,0:1], gyro_dataOut], axis=1)\n",
    "    gyro_data_filter_fft = np.concatenate([gyro_temp[:,0:1], gyro_dataOut_fft], axis=1)\n",
    "\n",
    "    gyro_df = pd.DataFrame.from_records(gyro_data_filter)\n",
    "    gyro_df_fft = pd.DataFrame.from_records(gyro_data_filter_fft)\n",
    "\n",
    "\n",
    "    euler_df = pd.read_csv(file_prefix + str(j) + '/Myo_orientationEuler.csv',index_col=None, header=None)\n",
    "\n",
    "    euler_temp = euler_df.values\n",
    "    euler_dataOut = np.zeros((len(euler_temp),3))\n",
    "    euler_dataOut_fft = np.zeros((len(euler_temp),3))\n",
    "    for p in range(3):\n",
    "        euler_dataOut[:,p] = butter_lowpass_filter(euler_df.iloc[:,p+1], cutoff, fss, order)\n",
    "        euler_dataOut[:,p] = euler_dataOut[:,p]/max(abs(euler_dataOut[:,p]))\n",
    "        euler_dataOut_fft[:,p] = np.fft.fft(euler_dataOut[:,p]).real\n",
    "\n",
    "    euler_data_filter = np.concatenate([euler_temp[:,0:1], euler_dataOut], axis=1)\n",
    "    euler_data_filter_fft = np.concatenate([euler_temp[:,0:1], euler_dataOut_fft], axis=1)\n",
    "\n",
    "\n",
    "    euler_df = pd.DataFrame.from_records(euler_data_filter)\n",
    "    euler_df_fft = pd.DataFrame.from_records(euler_data_filter_fft)\n",
    "\n",
    "\n",
    "    emg_df = pd.read_csv(file_prefix + str(j) + '/Myo_emg.csv',index_col=None, header=None)\n",
    "\n",
    "    temp = emg_df.values\n",
    "    dataOut = np.zeros((len(temp),8))\n",
    "    dataOut_fft = np.zeros((len(temp),8))\n",
    "    for p in range(8):\n",
    "        dataOut[:,p] = emg_butter_lowpass_filter(emg_df.iloc[:,p+1], emg_cutoff, fs, order) # ,emg_cutoff\n",
    "        dataOut[:,p] = dataOut[:,p]/max(abs(dataOut[:,p]))\n",
    "        dataOut_fft[:,p] = np.fft.fft(dataOut[:,p]).real\n",
    "\n",
    "    data_filter = np.concatenate([temp[:,0:1], dataOut], axis=1)\n",
    "    data_filter_fft = np.concatenate([temp[:,0:1], dataOut_fft], axis=1)\n",
    "\n",
    "    emg_df = pd.DataFrame.from_records(data_filter)\n",
    "    emg_df_fft = pd.DataFrame.from_records(data_filter_fft)\n",
    "\n",
    "    emg_df.to_csv(file_prefix + str(j) + '/emg_filter' + '.csv', index=False)\n",
    "    emg_df_fft.to_csv(file_prefix + str(j) + '/emg_fft' + '.csv', index=False)\n",
    "\n",
    "    gyro_df.to_csv(file_prefix + str(j) + '/gyro_filter' + '.csv', index=False)\n",
    "    gyro_df_fft.to_csv(file_prefix + str(j) + '/gyro_fft' + '.csv', index=False)\n",
    "\n",
    "    euler_df.to_csv(file_prefix + str(j) + '/orientationEuler_filter' + '.csv', index=False)\n",
    "    euler_df_fft.to_csv(file_prefix + str(j) + '/orientationEuler_fft' + '.csv', index=False)\n",
    "\n",
    "    acc_df.to_csv(file_prefix + str(j) + '/accelerometer_filter' + '.csv', index=False)\n",
    "    acc_df_fft.to_csv(file_prefix + str(j) + '/accelerometer_fft' + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Only Filtered Dataset (This cell should be re-run every time when the labels are changed)\n",
    "\n",
    "resample and segement window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\" read and label the datasets of only Filter data \"\"\"\n",
    "\n",
    "name = ['gyro_filter','orientationEuler_filter','accelerometer_filter']\n",
    "gyro = store_label_data(name[0])\n",
    "Myo_orientationEuler = store_label_data(name[1])\n",
    "Myo_accelerometer = store_label_data(name[2])\n",
    "imu_data = pd.concat([gyro.iloc[:,0:4],Myo_orientationEuler.iloc[:,1:4],Myo_accelerometer.iloc[:,1:5]],axis=1)\n",
    "imu_data = imu_data.iloc[:,1:]\n",
    "name = ['emg_filter']\n",
    "emg = store_label_data(name[0])\n",
    "emg_data = emg.iloc[:,1:]\n",
    "\n",
    "\"\"\"\" resample the EMG data into the same length of IMU data saved as resample_data \"\"\"\n",
    "f0 = signal.resample(emg_data.iloc[:,0], len(imu_data)) # resample data to imu numbers\n",
    "f1 = signal.resample(emg_data.iloc[:,1], len(imu_data))\n",
    "f2 = signal.resample(emg_data.iloc[:,2], len(imu_data))\n",
    "f3 = signal.resample(emg_data.iloc[:,3], len(imu_data))\n",
    "f4 = signal.resample(emg_data.iloc[:,4], len(imu_data))\n",
    "f5 = signal.resample(emg_data.iloc[:,5], len(imu_data))\n",
    "f6 = signal.resample(emg_data.iloc[:,6], len(imu_data))\n",
    "f7 = signal.resample(emg_data.iloc[:,7], len(imu_data))\n",
    "resample_data = np.concatenate([f0[:,np.newaxis], f1[:,np.newaxis], f2[:,np.newaxis],f3[:,np.newaxis],\n",
    "                                f4[:,np.newaxis], f5[:,np.newaxis], f6[:,np.newaxis], f7[:,np.newaxis]],axis=1)\n",
    "resample_data = pd.DataFrame(resample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\" set the window size and segement IMU and EMG respectively  \"\"\"\n",
    "window_size = 10\n",
    "\n",
    "# get the label and the label after encoding in one hot \n",
    "labels = extract_labels(imu_data,window_size)\n",
    "onehot_labels = np.asarray(pd.get_dummies(labels), dtype = np.int8)\n",
    "\n",
    "##############      IMU         #################\n",
    "data_len = imu_data.iloc[:,0].shape[0]\n",
    "reshaped_x1 = imu_data.iloc[:(data_len//window_size)*window_size,0].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x2 = imu_data.iloc[:(data_len//window_size)*window_size,1].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x3 = imu_data.iloc[:(data_len//window_size)*window_size,2].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x4 = imu_data.iloc[:(data_len//window_size)*window_size,3].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x5 = imu_data.iloc[:(data_len//window_size)*window_size,4].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x6 = imu_data.iloc[:(data_len//window_size)*window_size,5].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x7 = imu_data.iloc[:(data_len//window_size)*window_size,6].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x8 = imu_data.iloc[:(data_len//window_size)*window_size,7].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x9 = imu_data.iloc[:(data_len//window_size)*window_size,8].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_segments_imu = np.concatenate([reshaped_x1, reshaped_x2, reshaped_x3,reshaped_x4, reshaped_x5, reshaped_x6,\n",
    "                                   reshaped_x7, reshaped_x8, reshaped_x9],axis=2)\n",
    "\n",
    "##############      EMG        #################\n",
    "data_len = resample_data.iloc[:,0].shape[0]\n",
    "reshaped_x1 = resample_data.iloc[:(data_len//window_size)*window_size,0].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x2 = resample_data.iloc[:(data_len//window_size)*window_size,1].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x3 = resample_data.iloc[:(data_len//window_size)*window_size,2].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x4 = resample_data.iloc[:(data_len//window_size)*window_size,3].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x5 = resample_data.iloc[:(data_len//window_size)*window_size,4].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x6 = resample_data.iloc[:(data_len//window_size)*window_size,5].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x7 = resample_data.iloc[:(data_len//window_size)*window_size,6].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x8 = resample_data.iloc[:(data_len//window_size)*window_size,7].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_segments_emg = np.concatenate([reshaped_x1, reshaped_x2, reshaped_x3,reshaped_x4, reshaped_x5, reshaped_x6,\n",
    "                                   reshaped_x7, reshaped_x8],axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check the shape of each data and labels\n",
    "reshaped_segments_imu.shape,reshaped_segments_emg.shape,labels.shape,onehot_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Filtered and FFT Dataset (This cell should be re-run every time when the labels are changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\" read and label the datasets of Filtered and FFT data \"\"\"\n",
    "\n",
    "name = ['gyro_fft','orientationEuler_fft','accelerometer_fft']\n",
    "gyro_fft = store_label_data(name[0])\n",
    "Myo_orientationEuler_fft = store_label_data(name[1])\n",
    "Myo_accelerometer_fft = store_label_data(name[2])\n",
    "imu_data_fft = pd.concat([gyro_fft.iloc[:,0:4],Myo_orientationEuler_fft.iloc[:,1:4],Myo_accelerometer_fft.iloc[:,1:5]],axis=1)\n",
    "imu_data_fft = imu_data_fft.iloc[:,1:]\n",
    "name = ['emg_fft']\n",
    "emg_fft = store_label_data(name[0])\n",
    "emg_data_fft = emg_fft.iloc[:,1:]\n",
    "\n",
    "\"\"\"\" resample the EMG data into the same length of IMU data saved as resample_data \"\"\"\n",
    "f0 = signal.resample(emg_data_fft.iloc[:,0], len(imu_data_fft)) # resample data to imu numbers\n",
    "f1 = signal.resample(emg_data_fft.iloc[:,1], len(imu_data_fft))\n",
    "f2 = signal.resample(emg_data_fft.iloc[:,2], len(imu_data_fft))\n",
    "f3 = signal.resample(emg_data_fft.iloc[:,3], len(imu_data_fft))\n",
    "f4 = signal.resample(emg_data_fft.iloc[:,4], len(imu_data_fft))\n",
    "f5 = signal.resample(emg_data_fft.iloc[:,5], len(imu_data_fft))\n",
    "f6 = signal.resample(emg_data_fft.iloc[:,6], len(imu_data_fft))\n",
    "f7 = signal.resample(emg_data_fft.iloc[:,7], len(imu_data_fft))\n",
    "resample_data_fft = np.concatenate([f0[:,np.newaxis], f1[:,np.newaxis], f2[:,np.newaxis],f3[:,np.newaxis],\n",
    "                                f4[:,np.newaxis], f5[:,np.newaxis], f6[:,np.newaxis], f7[:,np.newaxis]],axis=1)\n",
    "resample_data_fft = pd.DataFrame(resample_data_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "\n",
    "##############      IMU         #################\n",
    "data_len = imu_data_fft.iloc[:,0].shape[0]\n",
    "reshaped_x1 = imu_data_fft.iloc[:(data_len//window_size)*window_size,0].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x2 = imu_data_fft.iloc[:(data_len//window_size)*window_size,1].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x3 = imu_data_fft.iloc[:(data_len//window_size)*window_size,2].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x4 = imu_data_fft.iloc[:(data_len//window_size)*window_size,3].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x5 = imu_data_fft.iloc[:(data_len//window_size)*window_size,4].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x6 = imu_data_fft.iloc[:(data_len//window_size)*window_size,5].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x7 = imu_data_fft.iloc[:(data_len//window_size)*window_size,6].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x8 = imu_data_fft.iloc[:(data_len//window_size)*window_size,7].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x9 = imu_data_fft.iloc[:(data_len//window_size)*window_size,8].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_segments_imu_fft = np.concatenate([reshaped_x1, reshaped_x2, reshaped_x3,reshaped_x4, reshaped_x5, reshaped_x6,\n",
    "                                   reshaped_x7, reshaped_x8, reshaped_x9],axis=2)\n",
    "\n",
    "##############      EMG        #################\n",
    "data_len = resample_data_fft.iloc[:,0].shape[0]\n",
    "reshaped_x1 = resample_data_fft.iloc[:(data_len//window_size)*window_size,0].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x2 = resample_data_fft.iloc[:(data_len//window_size)*window_size,1].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x3 = resample_data_fft.iloc[:(data_len//window_size)*window_size,2].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x4 = resample_data_fft.iloc[:(data_len//window_size)*window_size,3].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x5 = resample_data_fft.iloc[:(data_len//window_size)*window_size,4].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x6 = resample_data_fft.iloc[:(data_len//window_size)*window_size,5].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x7 = resample_data_fft.iloc[:(data_len//window_size)*window_size,6].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_x8 = resample_data_fft.iloc[:(data_len//window_size)*window_size,7].values.reshape([data_len//window_size,window_size,1])\n",
    "reshaped_segments_emg_fft = np.concatenate([reshaped_x1, reshaped_x2, reshaped_x3,reshaped_x4, reshaped_x5, reshaped_x6,\n",
    "                                   reshaped_x7, reshaped_x8],axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reshaped_segments_imu_fft.shape,reshaped_segments_emg_fft.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Feature extraction\n",
    "sma, mean, std, entropy, rms, zc, skew, kurt, pfd, meanfft, binpower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "itr = reshaped_segments_imu.shape[0]\n",
    "emg_sma = np.zeros((itr,1))\n",
    "\n",
    "emg_mean = np.zeros((itr,8))\n",
    "emg_zc = np.zeros((itr,8))\n",
    "emg_std = np.zeros((itr,8))\n",
    "emg_entropy = np.zeros((itr,8))\n",
    "emg_pfd = np.zeros((itr,8))\n",
    "emg_rms = np.zeros((itr,8))\n",
    "emg_kurt = np.zeros((itr,8))\n",
    "# emg_energy = np.zeros((itr,8))\n",
    "emg_skew = np.zeros((itr,8))\n",
    "\n",
    "a_emg_get_bin_power = np.zeros((itr,8))\n",
    "b_emg_get_bin_power = np.zeros((itr,8))\n",
    "emg_mean_fft = np.zeros((itr,8))\n",
    "emg_energy_fft = np.zeros((itr,8))\n",
    "\n",
    "ea_band = [1,8]\n",
    "eb_band = [9,16]\n",
    "\n",
    "label = np.zeros((itr,1))\n",
    "\n",
    "for i in range(itr):\n",
    "    emg_sma[i,:] = get_sma_numpy(reshaped_segments_emg[i,:,:])\n",
    "    for k in range(8):\n",
    "        emg_mean[i,k] = get_mean(reshaped_segments_emg[i,:,k])\n",
    "        emg_std[i,k] = get_std(reshaped_segments_emg[i,:,k]) # std 第二个 9-16 和 第四个 rms 25-32 和 第6个 41-48类似\n",
    "        emg_entropy[i,k] = get_entropy(reshaped_segments_emg[i,:,k]) # \n",
    "        emg_rms[i,k] = get_RMS(reshaped_segments_emg[i,:,k]) # std, rms 非常相似， 和skew有一部分相似\n",
    "        emg_zc[i,k] = get_ZC(reshaped_segments_emg[i,:,k]) # 17-24 and 33-40 nan\n",
    "        emg_skew[i,k] = get_skew(reshaped_segments_emg[i,:,k])\n",
    "        emg_kurt[i,k] = get_kurt(reshaped_segments_emg[i,:,k])\n",
    "        emg_pfd[i,k] = get_pfd(reshaped_segments_emg[i,:,k])\n",
    "           \n",
    "        emg_mean_fft[i,k] = get_mean(reshaped_segments_emg_fft[i,:,k])\n",
    "#         emg_energy_fft[i,k] = get_energy(reshaped_segments_emg_fft[i,:,k])\n",
    "        a_emg_get_bin_power[i,k] = get_bin_power(reshaped_segments_emg_fft[i,:,k],ea_band)\n",
    "        b_emg_get_bin_power[i,k] = get_bin_power(reshaped_segments_emg_fft[i,:,k],eb_band)  \n",
    "        \n",
    "emg_features = np.concatenate([emg_sma,emg_mean,emg_std,emg_entropy,emg_rms,emg_zc,emg_skew,emg_kurt,\n",
    "                               emg_pfd,emg_mean_fft,a_emg_get_bin_power,b_emg_get_bin_power], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "itr = reshaped_segments_imu.shape[0]\n",
    "gyro_sma = np.zeros((itr,1))\n",
    "orien_sma = np.zeros((itr,1))\n",
    "acc_sma = np.zeros((itr,1))\n",
    "imu_mean = np.zeros((itr,9))\n",
    "imu_std = np.zeros((itr,9))\n",
    "imu_pfd = np.zeros((itr,9))\n",
    "imu_rms = np.zeros((itr,9))\n",
    "imu_entropy = np.zeros((itr,9))\n",
    "imu_kurt = np.zeros((itr,9))\n",
    "imu_skew = np.zeros((itr,9))\n",
    "# imu_energy = np.zeros((itr,9))\n",
    "imu_zc = np.zeros((itr,9))\n",
    "\n",
    "a_imu_get_bin_power = np.zeros((itr,9))\n",
    "b_imu_get_bin_power = np.zeros((itr,9))\n",
    "imu_energy_fft = np.zeros((itr,9))\n",
    "imu_mean_fft = np.zeros((itr,9))\n",
    "label = np.zeros((itr,1))\n",
    "a_band = [1,8]\n",
    "b_band = [9,16]\n",
    "\n",
    "for i in range(itr):\n",
    "    gyro_sma[i,:] = get_sma_numpy(reshaped_segments_imu[i,:,:3])\n",
    "    orien_sma[i,:] = get_sma_numpy(reshaped_segments_imu[i,:,3:6])\n",
    "    acc_sma[i,:] = get_sma_numpy(reshaped_segments_imu[i,:,6:9]) # 0-2\n",
    "    for k in range(9):\n",
    "        imu_mean[i,k] = get_mean(reshaped_segments_imu[i,:,k]) # 3-12\n",
    "        imu_std[i,k] = get_std(reshaped_segments_imu[i,:,k]) # \n",
    "        imu_entropy[i,k] = get_entropy(reshaped_segments_imu[i,:,k]) # (21-29)and (39-47) \n",
    "        imu_rms[i,k] = get_RMS(reshaped_segments_imu[i,:,k]) # 这个feature 和imu_energy相似 (抛弃30-38)\n",
    "        imu_zc[i,k] = get_ZC(reshaped_segments_imu[i,:,k]) # zero和entropy存在nan的correlation关系 不知道为什么\n",
    "        imu_skew[i,k] = get_skew(reshaped_segments_imu[i,:,k]) \n",
    "        imu_kurt[i,k] = get_kurt(reshaped_segments_imu[i,:,k])\n",
    "        imu_pfd[i,k] = get_pfd(reshaped_segments_imu[i,:,k])\n",
    "        \n",
    "        imu_mean_fft[i,k] = get_mean(reshaped_segments_imu_fft[i,:,k]) # 12-21\n",
    "#         imu_energy_fft[i,k] = get_energy(reshaped_segments_imu_fft[i,:,k]) # 这个在SFS中被消除了，不需要这个feature.\n",
    "        a_imu_get_bin_power[i,k] = get_bin_power(reshaped_segments_imu_fft[i,:,k],a_band)\n",
    "        b_imu_get_bin_power[i,k] = get_bin_power(reshaped_segments_imu_fft[i,:,k],b_band)\n",
    "        \n",
    "        \n",
    "imu_features = np.concatenate([gyro_sma,orien_sma,acc_sma,imu_mean,imu_std,imu_entropy,imu_rms,imu_zc,\n",
    "                               imu_skew,imu_kurt,imu_pfd,imu_mean_fft,a_imu_get_bin_power,\n",
    "                               b_imu_get_bin_power], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# concatenate emg and imu features\n",
    "all_features = np.concatenate([emg_features,imu_features],axis=1)\n",
    "df_emg_features = pd.DataFrame(emg_features)\n",
    "df_imu_features = pd.DataFrame(imu_features)\n",
    "df_all_features = pd.DataFrame(all_features)\n",
    "df_onehot_labels = pd.DataFrame(onehot_labels)\n",
    "df_labels = pd.DataFrame(labels)\n",
    "\n",
    "\n",
    "\"\"\"\" If you want to store the features for convenience, then uncomment the three lines code below\"\"\"\n",
    "df_emg_features.to_csv('data/emg_features' + '.csv', index=False, header=None)\n",
    "df_imu_features.to_csv('data/imu_features' + '.csv', index=False, header=None)\n",
    "df_all_features.to_csv('data/all_features' + '.csv', index=False, header=None)\n",
    "df_onehot_labels.to_csv('data/onehotlabels' + '.csv', index=False, header=None)\n",
    "df_labels.to_csv('data/lables' + '.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data Spliting (IMU, EMG, Fusion)\n",
    "\n",
    "Here, it should take care of the labels format (one hot or decimal). <br/>\n",
    "\n",
    "one hot for LSTM model<br/>\n",
    "decimal for Classical models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "emg_features = pd.read_csv('data/emg_features.csv',index_col=None, header=None).values\n",
    "imu_features = pd.read_csv('data/imu_features.csv',index_col=None, header=None).values\n",
    "all_features = pd.read_csv('data/all_features.csv',index_col=None, header=None).values\n",
    "onehot_labels = pd.read_csv('data/onehotlabels.csv',index_col=None, header=None).values\n",
    "labels = pd.read_csv('data/lables.csv',index_col=None, header=None).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### IMU features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## splite the data into train and  test sets\n",
    "train_test_split_value = np.random.rand(len(imu_features)) < 0.8\n",
    "train_x = imu_features[train_test_split_value]\n",
    "test_x = imu_features[~train_test_split_value]\n",
    "\n",
    "\"\"\"\" one hot labels is used for the LSTM model \"\"\"\n",
    "train_y = onehot_labels[train_test_split_value]\n",
    "test_y = onehot_labels[~train_test_split_value]\n",
    "\n",
    "\"\"\"\" original labels is used for the other classical models \"\"\"\n",
    "class_train_y = labels[train_test_split_value]\n",
    "class_test_y = labels[~train_test_split_value]\n",
    "\n",
    "## splite the train set into train and  validation sets\n",
    "X_train, X_vld, y_train, y_vld = train_test_split(train_x, train_y, test_size=0.33, random_state=42)\n",
    "train_x.shape, X_vld.shape, test_x.shape, onehot_labels.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### EMG features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_test_split_value = np.random.rand(len(emg_features)) < 0.8\n",
    "train_x = emg_features[train_test_split_value]\n",
    "test_x = emg_features[~train_test_split_value]\n",
    "\n",
    "\"\"\"\" one hot labels is used for the LSTM model \"\"\"\n",
    "train_y = onehot_labels[train_test_split_value]\n",
    "test_y = onehot_labels[~train_test_split_value]\n",
    "\n",
    "\"\"\"\" original labels is used for the other classical models \"\"\"\n",
    "class_train_y = labels[train_test_split_value]\n",
    "class_test_y = labels[~train_test_split_value]\n",
    "\n",
    "X_train, X_vld, y_train, y_vld = train_test_split(train_x, train_y, test_size=0.33, random_state=42)\n",
    "train_x.shape, X_vld.shape, test_x.shape, onehot_labels.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Fusion features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_test_split_value = np.random.rand(len(all_features)) < 0.8\n",
    "train_x = all_features[train_test_split_value]\n",
    "test_x = all_features[~train_test_split_value]\n",
    "\n",
    "\"\"\"\" one hot labels is used for the LSTM model \"\"\"\n",
    "train_y = onehot_labels[train_test_split_value]\n",
    "test_y = onehot_labels[~train_test_split_value]\n",
    "\n",
    "\"\"\"\" original labels is used for the other classical models \"\"\"\n",
    "class_train_y = labels[train_test_split_value]\n",
    "class_test_y = labels[~train_test_split_value]\n",
    "\n",
    "X_train, X_vld, y_train, y_vld = train_test_split(train_x, train_y, test_size=0.33, random_state=42)\n",
    "train_x.shape, X_vld.shape, test_x.shape, onehot_labels.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "seq_len = 1\n",
    "r_len = X_train.shape[0]\n",
    "c_len = X_train.shape[1]\n",
    "X_train = X_train[:(r_len//seq_len)*seq_len,:].reshape([r_len//seq_len,seq_len,c_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r_len = X_vld.shape[0]\n",
    "c_len = X_vld.shape[1]\n",
    "X_vld = X_vld[:(r_len//seq_len)*seq_len,:].reshape([r_len//seq_len,seq_len,c_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r_len = test_x.shape[0]\n",
    "c_len = test_x.shape[1]\n",
    "test_x = test_x[:(r_len//seq_len)*seq_len,:].reshape([r_len//seq_len,seq_len,c_len])\n",
    "X_train.shape, X_vld.shape, test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When build the LSTM graph, it needs check the features 'n_channels' and 'n_classes'<br/>\n",
    "\n",
    "The default 'n_channels' is 191 features dimensions, and 'n_classes' is 4 for four classes classifiers, which corresponding to the 'Fusion features' cell from the above 'Data Spliting' fold.<br/>\n",
    "\n",
    "Then when you choose 'EMG features' from the above, you should also the the grapg setting to 'n_channels = 89' and 'lstm_size = 89*3', same for 'IMU'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lstm_size = 191*3         # 3 times the amount of channels\n",
    "lstm_layers = 3        # Number of layers\n",
    "batch_size = 900      # Batch size\n",
    "seq_len = 1 # 60         window size  # Number of steps / how many input samples  \n",
    "learning_rate = 0.0001  # Learning rate (default is 0.001)\n",
    "epochs = 100\n",
    "\n",
    "# Fixed\n",
    "n_classes = 4        # number of classifiers\n",
    "n_channels = 191         # dimensions of input\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "# Construct placeholders\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs')\n",
    "    labels_ = tf.placeholder(tf.float32, [None, n_classes], name = 'labels')\n",
    "    keep_prob_ = tf.placeholder(tf.float32, name = 'keep')\n",
    "    learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate')\n",
    "\n",
    "    # Construct the LSTM inputs and LSTM cells\n",
    "    lstm_in = tf.transpose(inputs_, [1,0,2]) # reshape into (seq_len, N, channels)\n",
    "    lstm_in = tf.reshape(lstm_in, [-1, n_channels]) # Now (seq_len*N, n_channels)\n",
    "    \n",
    "    # To cells\n",
    "    lstm_in = tf.layers.dense(lstm_in, lstm_size, activation=None) # or tf.nn.relu, tf.nn.sigmoid, tf.nn.tanh?\n",
    "    \n",
    "    # Open up the tensor into a list of seq_len pieces\n",
    "    lstm_in = tf.split(lstm_in, seq_len, 0)\n",
    "    \n",
    "    # Add LSTM layers\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    \n",
    "    outputs, final_state = tf.contrib.rnn.static_rnn(cell, lstm_in, dtype=tf.float32,\n",
    "                                                     initial_state = initial_state)\n",
    "    \n",
    "    # We only need the last output tensor to pass into a classifier\n",
    "    logits = tf.layers.dense(outputs[-1], n_classes, name='logits')\n",
    "    \n",
    "    # Cost function and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_))\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost) # No grad clipping\n",
    "    \n",
    "    # Grad clipping\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate_)\n",
    "\n",
    "    gradients = train_op.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "    optimizer = train_op.apply_gradients(capped_gradients)\n",
    "    \n",
    "    # Accuracy\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "    y_pred = tf.argmax(logits, 1)\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if (os.path.exists('checkpoints') == False):\n",
    "    !mkdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "validation_acc = []\n",
    "validation_loss = []\n",
    "validation_f1score = []\n",
    "train_f1score = []\n",
    "\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "train_pred = []\n",
    "train_true = []\n",
    "tra_true = []\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    count = 0\n",
    "    save_count = 0\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        # Initialize \n",
    "        state = sess.run(initial_state)\n",
    "        # Loop over batches\n",
    "        for x, y in get_batches(X_train, y_train, batch_size):\n",
    "            # Feed dictionary\n",
    "            feed = {inputs_ : x, labels_ : y, keep_prob_ : 0.5, \n",
    "                    initial_state : state, learning_rate_ : learning_rate}\n",
    "            \n",
    "            loss, _ , state, acc, yt_pred = sess.run([cost, optimizer, final_state, accuracy, y_pred], \n",
    "                                             feed_dict = feed)\n",
    "            \n",
    "            train_true = np.argmax(y, axis=1)\n",
    "            tr_f1score = f1_score(train_true, yt_pred, average='micro')\n",
    "            train_f1score.append(tr_f1score)\n",
    "            tra_true.append(y)\n",
    "            train_pred.append(yt_pred)\n",
    "            train_acc.append(acc)\n",
    "            train_loss.append(loss)\n",
    "#             training_f1score.append(np.mean(train_f1score))\n",
    "            # Print at each 5 iters\n",
    "            if (iteration % 5 == 0):\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {:d}\".format(iteration),\n",
    "                      \"Train loss: {:6f}\".format(loss),\n",
    "                      \"Train acc: {:.6f}\".format(acc))\n",
    "            \n",
    "#             Compute validation loss at every 25 iterations\n",
    "            if (iteration%25 == 0):\n",
    "                \n",
    "                # Initiate for validation set\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                \n",
    "                val_f1score = []\n",
    "                val_acc_ = []\n",
    "                val_loss_ = []\n",
    "                val_pred = []\n",
    "                val_true = []\n",
    "                for x_v, y_v in get_batches(X_vld, y_vld, batch_size):\n",
    "                    # Feed\n",
    "                    feed = {inputs_ : x_v, labels_ : y_v, keep_prob_ : 1.0, initial_state : val_state}\n",
    "                    \n",
    "                    # Loss\n",
    "                    loss_v, state_v, acc_v,yv_pred= sess.run([cost, final_state, accuracy, y_pred], feed_dict = feed)\n",
    "                    \n",
    "#                     if (acc_v > val_acc_[-1]) & count > 65:\n",
    "# #                         saver.save(sess,\"checkpoints/har-lstm.ckpt\")\n",
    "#                         save_count += 1\n",
    "                    \n",
    "                    valid_true = np.argmax(y_v, axis=1)\n",
    "                    f1score = f1_score(valid_true, yv_pred, average='micro')\n",
    "                    val_f1score.append(f1score)\n",
    "                    val_true.append(y_v)\n",
    "                    val_pred.append(yv_pred)\n",
    "                    val_acc_.append(acc_v)\n",
    "                    val_loss_.append(loss_v)\n",
    "                    \n",
    "                # Print info\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {:d}\".format(iteration),\n",
    "                      \"Validation loss: {:6f}\".format(np.mean(val_loss_)),\n",
    "                      \"Validation acc: {:.6f}\".format(np.mean(val_acc_)))\n",
    "                \n",
    "                # Store\n",
    "                validation_acc.append(np.mean(val_acc_))\n",
    "                validation_loss.append(np.mean(val_loss_))\n",
    "                validation_f1score.append(np.mean(val_f1score))\n",
    "            # Iterate \n",
    "            iteration += 1\n",
    "#     saver.save(sess,\"checkpoints/har-lstm.ckpt\") # 10 imu cutoff,20 emg cutoff, 30 window size, 5 classes\n",
    "    saver.save(sess,\"checkpoints/har-lstm.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### LSTM analysis\n",
    "\n",
    "plot the confusion matrix,f1-score and auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_pred =list(chain.from_iterable(train_pred))\n",
    "tra_true =list(chain.from_iterable(tra_true[-2800:]))\n",
    "tra_true = np.argmax(tra_true, axis=1)\n",
    "confusion_m = confusion_matrix(tra_true[-2800:], train_pred[-2800:]) \n",
    "confusion_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot training and test loss\n",
    "t = np.arange(iteration-1)\n",
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.plot(t, np.array(train_loss), 'r-', t[t % 25 == 0], np.array(validation_loss), 'b-')\n",
    "plt.xlabel(\"iteration\",fontsize=13)\n",
    "plt.ylabel(\"Loss\",fontsize=13)\n",
    "plt.title('Plot training and test loss',fontsize=18)\n",
    "plt.legend(['train', 'validation'], loc='upper right',fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot Accuracies\n",
    "plt.figure(figsize = (6,6))\n",
    "\n",
    "# plt.plot(t[0:30], np.array(train_acc)[0:30], 'r-')\n",
    "plt.plot(t, np.array(train_f1score), 'r-', t[t % 25 == 0], validation_f1score, 'b-')\n",
    "plt.xlabel(\"iteration\",fontsize=13)\n",
    "plt.ylabel(\"F1-score\",fontsize=13)\n",
    "plt.title('Plot F1-score',fontsize=18)\n",
    "plt.legend(['train', 'validation'], loc='lower right',fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_acc = []\n",
    "test_pred = []\n",
    "test_f1score = []\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # Restore\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    \n",
    "    for x_t, y_t in get_batches(test_x, test_y, batch_size):\n",
    "        feed = {inputs_: x_t,\n",
    "                labels_: y_t,\n",
    "                keep_prob_: 1,\n",
    "                initial_state: test_state}\n",
    "        \n",
    "        batch_acc, test_state, t_pred = sess.run([accuracy, final_state, y_pred], feed_dict=feed)\n",
    "        \n",
    "        test_true = np.argmax(y_t, axis=1)\n",
    "        t_f1score = f1_score(test_true, t_pred, average='micro')\n",
    "        test_f1score.append(t_f1score)\n",
    "        test_pred.append(t_pred)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test F1-score: {:.6f}\".format(np.mean(test_f1score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_true = label_binarize(test_true, classes=[0,1,2,3])\n",
    "t_pred = label_binarize(t_pred, classes=[0,1,2,3])\n",
    "n_classes = 4\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(test_true[:, i], t_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot of a ROC curve for a specific class\n",
    "for i in range(n_classes):\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(test_true.ravel(), t_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Uncomment below codes for plot all ROC curves\n",
    "plt.figure()\n",
    "# plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "#          label='micro-average ROC curve (area = {0:0.2f})'\n",
    "#                ''.format(roc_auc[\"micro\"]),\n",
    "#          color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "# plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "#          label='macro-average ROC curve (area = {0:0.2f})'\n",
    "#                ''.format(roc_auc[\"macro\"]),\n",
    "#          color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, linewidth=4,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_len = (test_x.shape[0]//batch_size)*batch_size\n",
    "test_pred =list(chain.from_iterable(test_pred))\n",
    "test_true = np.argmax(test_y, axis=1)\n",
    "confusion_m = confusion_matrix(test_true[:test_len], test_pred)\n",
    "confusion_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.matshow(confusion_m, cmap=plt.cm.Blues)\n",
    "\n",
    "for x in range(len(confusion_m)): #数据标签\n",
    "    for y in range(len(confusion_m)):\n",
    "        plt.annotate(confusion_m[x,y], xy=(x, y), horizontalalignment='center', verticalalignment='center')\n",
    "plt.colorbar() #颜色标签\n",
    "plt.ylabel('True label') #坐标轴标签\n",
    "plt.xlabel('Predicted label') #坐标轴标签\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Classical Models\n",
    "\n",
    "All the models used 5 cross validation folds to aviod overfitting<br/>\n",
    "\n",
    "The F1-score is calculated by the 'macro', and display the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# X = train_x  # X = test_x\n",
    "# biy = class_train_y  # biy = class_test_y\n",
    "\n",
    "X = test_x\n",
    "biy = class_test_y\n",
    "biy = np.squeeze(biy, axis=(1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Cs=[0.001,0.01,0.1,1,10,100,1000,10000]\n",
    "bi_train_f1s=[]\n",
    "bi_test_f1s=[]\n",
    "\n",
    "max_c_bi=-1\n",
    "max_test_f1_bi = 0\n",
    "\n",
    "for c in Cs:\n",
    "    clf = LogisticRegression(C=c,solver='liblinear')\n",
    "    [train_acc,test_acc,train_f1,test_f1,confusion_m] = getF1Score(X,biy,clf)\n",
    "    bi_train_f1s.append(train_f1)\n",
    "    bi_test_f1s.append(test_f1)\n",
    "    if (test_f1>max_test_f1_bi):\n",
    "        max_test_f1_bi = test_f1\n",
    "        max_c_bi=c\n",
    "x = range(len(Cs))\n",
    "plt.plot(x, bi_train_f1s, 'ro--',label='train F1')\n",
    "plt.plot(x, bi_test_f1s, 'go--',label='test F1')\n",
    "plt.xticks(x, Cs)\n",
    "plt.title('LogisticRegression with liblinear solver')\n",
    "plt.xlabel('c')\n",
    "plt.legend();    \n",
    "plt.show();    \n",
    "\n",
    "print('best paras for bi levels:')\n",
    "print('c:'+str(max_c_bi)+' max_test_f1_bi:'+str(max_test_f1_bi))\n",
    "print('confusion_matrix:')\n",
    "print(confusion_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "As=['euclidean', 'manhattan','chebyshev','minkowski']\n",
    "train_f1s=[]\n",
    "test_f1s=[]\n",
    "bi_train_f1s=[]\n",
    "bi_test_f1s=[]\n",
    "max_k='auto'\n",
    "max_k_bi='auto'\n",
    "max_test_f1 = 0\n",
    "max_test_f1_bi = 0\n",
    "for k in As:\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors = 10 , metric=k)\n",
    "    [train_acc,test_acc,train_f1,test_f1,confusion_m] = getF1Score(X,biy,clf)\n",
    "    bi_train_f1s.append(train_f1)\n",
    "    bi_test_f1s.append(test_f1)\n",
    "    \n",
    "    if (test_f1>max_test_f1_bi):\n",
    "        max_test_f1_bi = test_f1\n",
    "        max_k_bi=k\n",
    "        \n",
    "x = range(len(As))\n",
    "plt.plot(x, bi_train_f1s, 'ro--',label='binary train F1')\n",
    "plt.plot(x, bi_test_f1s, 'go--',label='binary test F1')\n",
    "plt.xticks(x, As)\n",
    "plt.xlabel('distance matric')\n",
    "plt.title('KNN for different distance matric')\n",
    "plt.legend();    \n",
    "plt.show();\n",
    "\n",
    "print('best parameters for bi levels:')\n",
    "print('metric:'+max_k_bi+', max_test_f1_bi:'+str(max_test_f1_bi))\n",
    "print('confusion_matrix:')\n",
    "print(confusion_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# adjust the metric method according the best result get from the above cell\n",
    "As=[10,50,100,150]\n",
    "train_f1s=[]\n",
    "test_f1s=[]\n",
    "bi_train_f1s=[]\n",
    "bi_test_f1s=[]\n",
    "max_k='auto'\n",
    "max_k_bi='auto'\n",
    "max_test_f1 = 0\n",
    "max_test_f1_bi = 0\n",
    "for k in As:\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors = k , metric='manhattan')\n",
    "    [train_acc,test_acc,train_f1,test_f1,confusion_m] = getF1Score(X,biy,clf)\n",
    "    bi_train_f1s.append(train_f1)\n",
    "    bi_test_f1s.append(test_f1)\n",
    "    \n",
    "    if (test_f1>max_test_f1_bi):\n",
    "        max_test_f1_bi = test_f1\n",
    "        max_k_bi=k\n",
    "        \n",
    "x = range(len(As))\n",
    "plt.plot(x, bi_train_f1s, 'ro--',label='binary train F1')\n",
    "plt.plot(x, bi_test_f1s, 'go--',label='binary test F1')\n",
    "plt.xticks(x, As)\n",
    "plt.xlabel('distance matric')\n",
    "plt.title('KNN for different distance matric')\n",
    "plt.legend();    \n",
    "plt.show();\n",
    "\n",
    "print('best parameters for bi levels:')\n",
    "print('metric:'+str(max_k_bi)+', max_test_f1_bi:'+str(max_test_f1_bi))\n",
    "print('confusion_matrix:')\n",
    "print(confusion_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max_depths=[1,2,3,4,5,6,7,8,9,10]#,11,12,13,14,15,16,17,18,19,20,21]\n",
    "train_f1s=[]\n",
    "test_f1s=[]\n",
    "bi_train_f1s=[]\n",
    "bi_test_f1s=[]\n",
    "max_c=-1\n",
    "max_c_bi=-1\n",
    "max_test_f1 = 0\n",
    "max_test_f1_bi = 0\n",
    "for max_depth in max_depths:\n",
    "    clf = RandomForestClassifier(max_depth=max_depth,n_estimators=6, random_state=0)\n",
    "    [train_acc,test_acc,train_f1,test_f1,confusion_m] = getF1Score(X,biy,clf)\n",
    "    bi_train_f1s.append(train_f1)\n",
    "    bi_test_f1s.append(test_f1)\n",
    "    if (test_f1>max_test_f1_bi):\n",
    "        max_test_f1_bi = test_f1\n",
    "        max_c_bi=max_depth\n",
    "x = range(len(max_depths))\n",
    "plt.plot(x, bi_train_f1s, 'ro--',label='binary train F1')\n",
    "plt.plot(x, bi_test_f1s, 'go--',label='binary test F1')\n",
    "plt.xticks(x, max_depths)\n",
    "plt.title('Random Forest for different max_depths')\n",
    "plt.legend();    \n",
    "plt.show();    \n",
    "\n",
    "print('best para for bi levels:')\n",
    "print('max_depth:'+str(max_c_bi)+' max_test_f1_bi:'+str(max_test_f1_bi))\n",
    "print('confusion_matrix:')\n",
    "print(confusion_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# same as KNN, choose the best result get from the above for the 'max_depth'\n",
    "ns=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21]\n",
    "train_f1s=[]\n",
    "test_f1s=[]\n",
    "bi_train_f1s=[]\n",
    "bi_test_f1s=[]\n",
    "max_n=-1\n",
    "max_n_bi=-1\n",
    "max_test_f1 = 0\n",
    "max_test_f1_bi = 0\n",
    "for n in ns:\n",
    "    clf = RandomForestClassifier(max_depth=7,n_estimators=n,random_state=0)\n",
    "    [train_acc,test_acc,train_f1,test_f1,confusion_m] = getF1Score(X,biy,clf)\n",
    "    bi_train_f1s.append(train_f1)\n",
    "    bi_test_f1s.append(test_f1)\n",
    "    if (test_f1>max_test_f1_bi):\n",
    "        max_test_f1_bi = test_f1\n",
    "        max_n_bi=n\n",
    "x = range(len(ns))\n",
    "plt.plot(x, bi_train_f1s, 'ro--',label='binary train F1')\n",
    "plt.plot(x, bi_test_f1s, 'go--',label='binary test F1')\n",
    "plt.xticks(x, ns)\n",
    "plt.title('Random Forest for different n_estimators')\n",
    "plt.legend();    \n",
    "plt.show(); \n",
    "\n",
    "print('best para for bi levels:')\n",
    "print('n:'+str(max_n_bi)+' max_test_f1_bi:'+str(max_test_f1_bi))\n",
    "print('confusion_matrix:')\n",
    "print(confusion_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf = LinearDiscriminantAnalysis()\n",
    "[train_acc,test_acc,train_f1,test_f1,confusion_m] = getF1Score(X,biy,clf)\n",
    "print('confusion_matrix:')\n",
    "print(confusion_m)\n",
    "print(test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’\n",
    "Cs=[0.001,0.01,0.1,0.2]\n",
    "train_f1s=[]\n",
    "test_f1s=[]\n",
    "bi_train_f1s=[]\n",
    "bi_test_f1s=[]\n",
    "max_c=-1\n",
    "max_c_bi=-1\n",
    "max_test_f1 = 0\n",
    "max_test_f1_bi = 0\n",
    "for c in Cs:\n",
    "    clf = svm.SVC(C=c, class_weight='balanced', kernel='linear', random_state=0)\n",
    "    [train_acc,test_acc,train_f1,test_f1,confusion_m] = getF1Score(X,biy,clf)\n",
    "    bi_train_f1s.append(train_f1)\n",
    "    bi_test_f1s.append(test_f1)\n",
    "    if (test_f1>max_test_f1_bi):\n",
    "        max_test_f1_bi = test_f1\n",
    "        max_c_bi=c\n",
    "x = range(len(Cs))\n",
    "plt.plot(x, bi_train_f1s, 'ro--',label='binary train F1')\n",
    "plt.plot(x, bi_test_f1s, 'go--',label='binary test F1')\n",
    "plt.xticks(x, Cs)\n",
    "plt.xlabel('c')\n",
    "plt.title('SVC with linear kernel')\n",
    "plt.legend();    \n",
    "plt.show();    \n",
    "\n",
    "print('best para for bi levels:')\n",
    "print('c:'+str(max_c_bi)+' max_test_f1_bi:'+str(max_test_f1_bi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# non linear model which takes very long time‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’\n",
    "Cs=[0.001,0.01,0.1,1,10,100]\n",
    "train_f1s=[]\n",
    "test_f1s=[]\n",
    "bi_train_f1s=[]\n",
    "bi_test_f1s=[]\n",
    "max_c=-1\n",
    "max_c_bi=-1\n",
    "max_test_f1 = 0\n",
    "max_test_f1_bi = 0\n",
    "for c in Cs:\n",
    "    clf = svm.SVC(C=c, class_weight='balanced', kernel='sigmoid', random_state=0)\n",
    "    [train_acc,test_acc,train_f1,test_f1,confusion_m] = getF1Score(X,biy,clf)\n",
    "    bi_train_f1s.append(train_f1)\n",
    "    bi_test_f1s.append(test_f1)\n",
    "    if (test_f1>max_test_f1_bi):\n",
    "        max_test_f1_bi = test_f1\n",
    "        max_c_bi=c\n",
    "x = range(len(Cs))\n",
    "plt.plot(x, bi_train_f1s, 'ro--',label='binary train F1')\n",
    "plt.plot(x, bi_test_f1s, 'go--',label='binary test F1')\n",
    "plt.xticks(x, Cs)\n",
    "plt.xlabel('c')\n",
    "plt.title('SVC with sigmoid kernel')\n",
    "plt.legend();    \n",
    "plt.show();\n",
    "\n",
    "print('best para for bi levels:')\n",
    "print('c:'+str(max_c_bi)+' max_test_f1_bi:'+str(max_test_f1_bi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Sequential Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=10,solver='liblinear')\n",
    "# lr = svm.SVC(C=c, class_weight='balanced', kernel='linear', random_state=0)\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "sfs1 = SFS(lr, \n",
    "           k_features= 191, \n",
    "           forward=True,\n",
    "           floating=False, \n",
    "           verbose=2,\n",
    "           scoring='accuracy',\n",
    "           cv=0,\n",
    "           n_jobs=1)\n",
    "\n",
    "sfs1 = sfs1.fit(X, biy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(sfs1.get_metric_dict()).T\n",
    "sor = df.sort_values('cv_scores')\n",
    "sor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig = plot_sfs(sfs1.get_metric_dict(), kind='std_dev')\n",
    "# fig(figsize=[12,12])\n",
    "plt.ylim([0.5, 1])\n",
    "plt.xlim([20,40])\n",
    "plt.title('Sequential Forward Selection (w. StdDev)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
